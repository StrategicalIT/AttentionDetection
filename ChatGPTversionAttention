import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_face_mesh = mp.solutions.face_mesh

# Load video file
cap = cv2.VideoCapture('video_file.mp4')

# Initialize Mediapipe Face Mesh model
with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
    while cap.isOpened():
        # Read frame from video file
        success, image = cap.read()
        if not success:
            break

        # Convert image to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Detect face landmarks
        results = face_mesh.process(image)

        # Check if face landmarks are detected
        if results.multi_face_landmarks:
            # Extract face landmarks
            face_landmarks = results.multi_face_landmarks[0]

            # Calculate eye aspect ratio (EAR)
            left_eye = [
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.LEFT_EYE_TOP].x,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.LEFT_EYE_TOP].y,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.LEFT_EYE_BOTTOM].x,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.LEFT_EYE_BOTTOM].y,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.LEFT_EYE_LEFT].x,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.LEFT_EYE_RIGHT].x
            ]
            right_eye = [
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.RIGHT_EYE_TOP].x,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.RIGHT_EYE_TOP].y,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.RIGHT_EYE_BOTTOM].x,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.RIGHT_EYE_BOTTOM].y,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.RIGHT_EYE_LEFT].x,
                face_landmarks.landmark[mp_face_mesh.FaceLandmark.RIGHT_EYE_RIGHT].x
            ]
            left_ear = (abs(left_eye[1] - left_eye[3]) + abs(left_eye[4] - left_eye[5])) / (2 * abs(left_eye[0] - left_eye[2]))
            right_ear = (abs(right_eye[1] - right_eye[3]) + abs(right_eye[4] - right_eye[5])) / (2 * abs(right_eye[0] - right_eye[2]))
            ear = (left_ear + right_ear) / 2

            # Check if person is distracted
            if ear < 0.2:
                print('Person is distracted')
            else:
                print('Person is paying attention')

        # Draw face landmarks on image
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        mp_drawing.draw_landmarks(
            image=image,
            landmark_list=results.multi_face_landmarks[0],
            connections=mp_face_mesh.FACEMESH_CONTOURS,
            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),
            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1))

        # Show image
        cv2.imshow('MediaPipe Face Mesh', image)
        if cv2.waitKey
